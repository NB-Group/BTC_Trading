# btc_predictor/optimize.py

import torch
import numpy as np
import pandas as pd
import json
import os
import optuna
import torch.nn as nn
import argparse
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import f1_score, mean_squared_error
import torch.optim as optim

from .config import get_model_config, PATHS
from .data import get_prepared_data_for_training
from .model import create_model
from .utils import LOGGER, setup_logger

# --- å…¨å±€å¸¸é‡ ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
N_TRIALS = 50 # Optuna ä¼˜åŒ–çš„è¯•éªŒæ¬¡æ•°

def objective(trial: optuna.Trial, model_name: str, X_train, y_train, X_val, y_val) -> float:
    """
    Optuna çš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºå¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°ã€‚
    """
    model_config = get_model_config(model_name)
    task = model_config.get('task', 'regression')

    # --- 1. å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆæ‰©å±•ç‰ˆï¼‰ ---
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True), # å­¦ä¹ ç‡èŒƒå›´æ›´å¹¿
        'nhead': trial.suggest_categorical('nhead', [2, 4, 8, 16]), # å¢åŠ æ›´å¤šå¤´
        'num_encoder_layers': trial.suggest_int('num_encoder_layers', 1, 6), # æ›´æ·±çš„æ¨¡å‹
        'dim_feedforward': trial.suggest_int('dim_feedforward', 128, 1024, step=128), # æ›´å®½çš„ç½‘ç»œ
        'dropout': trial.suggest_float('dropout', 0.1, 0.5),
        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256]), # æ›´å¤šæ‰¹æ¬¡å¤§å°é€‰é¡¹
    }

    # --- 2. æ•°æ®å‡†å¤‡å’ŒåŠ è½½ ---
    X_train_scaled = torch.tensor(X_train, dtype=torch.float32)
    y_train_scaled = torch.tensor(y_train, dtype=torch.float32)
    X_val_scaled = torch.tensor(X_val, dtype=torch.float32)
    y_val_scaled = torch.tensor(y_val, dtype=torch.float32)

    train_dataset = TensorDataset(X_train_scaled, y_train_scaled)
    val_dataset = TensorDataset(X_val_scaled, y_val_scaled)
    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)
    
    # --- 3. æ¨¡å‹å’ŒæŸå¤±å‡½æ•° ---
    model_params = {
        'input_dim': X_train.shape[1],
        'task': task,
        'model_type': model_config.get('model_type', 'transformer'),
        **params
    }
    model = create_model(**model_params).to(DEVICE)
    
    if task == 'regression':
        criterion = nn.MSELoss()
        eval_metric = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)) # RMSE
    else: # classification
        criterion = nn.BCELoss()
        eval_metric = f1_score
        
    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])

    # --- 4. è®­ç»ƒå’ŒéªŒè¯å¾ªç¯ ---
    for epoch in range(25): # åœ¨ä¼˜åŒ–ä¸­å¢åŠ å‘¨æœŸä»¥è·å¾—æ›´å¯é çš„ç»“æœ
        model.train()
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

    # --- 5. è¯„ä¼° ---
    model.eval()
    y_preds = []
    y_trues = []
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
            outputs = model(batch_X)
            y_preds.extend(outputs.cpu().numpy())
            y_trues.extend(batch_y.cpu().numpy())

    if task == 'regression':
        # æ˜ç¡®å°†numpy.float64è½¬æ¢ä¸ºpython float
        return float(eval_metric(y_trues, y_preds))
    else: # classification
        y_preds_binary = (np.array(y_preds) > 0.5).astype(int)
        # æ˜ç¡®å°†numpy.float64è½¬æ¢ä¸ºpython float
        return float(eval_metric(y_trues, y_preds_binary))


def run_optimization(model_name: str):
    """
    ä¸ºæŒ‡å®šæ¨¡å‹è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
    """
    setup_logger()
    LOGGER.info(f"ğŸš€ å¼€å§‹ä¸ºæ¨¡å‹ '{model_name}' è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–...")

    # 1. åŠ è½½æ•°æ®
    X, y = get_prepared_data_for_training(model_name)
    if X.empty:
        LOGGER.error("æ•°æ®ä¸ºç©ºï¼Œä¼˜åŒ–ä¸­æ­¢ã€‚")
        return

    # 2. æ•°æ®æ‹†åˆ†å’Œç¼©æ”¾ï¼ˆä¸ train.py ä¿æŒä¸€è‡´ï¼‰
    tscv = TimeSeriesSplit(n_splits=5)
    for train_index, val_index in tscv.split(X):
        pass # è·å–æœ€åä¸€ä¸ªæ‹†åˆ†
    
    X_train, X_val = X.iloc[train_index, :], X.iloc[val_index,:]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)

    task = get_model_config(model_name).get('task', 'regression')
    if task == 'regression':
        scaler_y = StandardScaler()
        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
        y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))
    else:
        y_train_scaled, y_val_scaled = y_train.values, y_val.values

    # 3. è¿è¡Œ Optuna ç ”ç©¶
    study_direction = 'minimize' if task == 'regression' else 'maximize'
    study = optuna.create_study(direction=study_direction)
    
    study.optimize(
        lambda trial: objective(trial, model_name, X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled),
        n_trials=None, # ç§»é™¤å›ºå®šè¯•éªŒæ¬¡æ•°
        timeout=36000, # è®¾ç½®10å°æ—¶è¿è¡Œæ—¶é—´
        show_progress_bar=True
    )

    # 4. ä¿å­˜æœ€ä¼˜å‚æ•°
    best_params = study.best_params
    params_path = os.path.join(PATHS['results'], f"best_params_{model_name}.json")
    with open(params_path, 'w') as f:
        json.dump(best_params, f, indent=4)
        
    LOGGER.success(f"ä¼˜åŒ–å®Œæˆï¼æœ€ä½³å‚æ•°å·²ä¿å­˜è‡³ {params_path}")
    LOGGER.info(f"æœ€ä½³è¯•éªŒåˆ†æ•°ï¼ˆ{'RMSE' if task == 'regression' else 'F1åˆ†æ•°'}ï¼‰: {study.best_value}")
    LOGGER.info(f"æœ€ä½³å‚æ•°: {best_params}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¿è¡Œæ¨¡å‹çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚")
    parser.add_argument("--model-name", required=True, type=str, help="è¦ä¼˜åŒ–çš„æ¨¡å‹åç§°ã€‚")
    args = parser.parse_args()
    run_optimization(args.model_name)