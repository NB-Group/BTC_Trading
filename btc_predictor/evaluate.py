# btc_predictor/evaluate.py

import os
import torch
import joblib
import numpy as np
import pandas as pd
import json
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

from .config import PATHS, DEVICE, get_model_config
from .data import get_prepared_data_for_training
from .utils import LOGGER, setup_logger, load_model_artifacts, RegressionDataset
from torch.utils.data import DataLoader

def run_evaluation(model_name: str):
    """
    åŠ è½½å·²è®­ç»ƒçš„äº‹ä»¶é©±åŠ¨æ¨¡å‹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨ã€‚
    """
    setup_logger()
    LOGGER.info(f"ğŸš€ å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")

    # --- 1. åŠ è½½æ¨¡å‹å’Œå·¥ä»¶ ---
    try:
        artifacts = load_model_artifacts(model_name)
        model = artifacts['model']
        scaler_X = artifacts['scaler_X']
        scaler_y = artifacts.get('scaler_y') # å¦‚æœæ˜¯å›å½’ä»»åŠ¡ï¼Œåˆ™åº”è¯¥å­˜åœ¨
        params = artifacts['params']
    except Exception as e:
        LOGGER.error(f"åŠ è½½æ¨¡å‹å·¥ä»¶å¤±è´¥: {e}ã€‚è¯·å…ˆè¿è¡Œ train.pyã€‚")
        return

    if not scaler_y:
        LOGGER.error("æ¨¡å‹æ˜¯å›å½’ä»»åŠ¡ï¼Œä½†æœªæ‰¾åˆ° y-scaler (scaler_y)ã€‚æ— æ³•ç»§ç»­è¯„ä¼°ã€‚")
        return
        
    LOGGER.info("æ¨¡å‹å’Œå·¥ä»¶åŠ è½½æˆåŠŸã€‚")

    # --- 2. å‡†å¤‡è¯„ä¼°æ•°æ® (æµ‹è¯•é›†) ---
    LOGGER.info("æ­£åœ¨å‡†å¤‡æµ‹è¯•æ•°æ®é›†...")
    X, y = get_prepared_data_for_training(model_name)
    if X.empty:
        LOGGER.error("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°ã€‚")
        return

    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹å¼åˆ†å‰²æ•°æ®ä»¥è·å–æµ‹è¯•é›†
    n_splits = get_model_config(model_name).get('n_splits', 5)
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    # è·å–æœ€åä¸€ä¸ª (train_index, test_index) åˆ†å‰²
    all_splits = list(tscv.split(X))
    train_index, test_index = all_splits[-1]
    
    X_test, y_test = X.iloc[test_index], y.iloc[test_index]
    
    # å¯¹æµ‹è¯•é›†è¿›è¡Œç¼©æ”¾
    X_test_scaled = scaler_X.transform(X_test)
    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))

    # åˆ›å»ºDataLoader
    test_dataset = RegressionDataset(
        torch.tensor(X_test_scaled, dtype=torch.float32), 
        torch.tensor(y_test_scaled, dtype=torch.float32).squeeze()
    )
    test_loader = DataLoader(test_dataset, batch_size=params.get('batch_size', 32), shuffle=False)
    
    LOGGER.info(f"å·²å‡†å¤‡ {len(test_dataset)} ä¸ªæ ·æœ¬ç”¨äºæœ€ç»ˆè¯„ä¼°ã€‚")

    # --- 3. æ‰§è¡Œé¢„æµ‹ ---
    model.eval()
    all_preds_scaled = []
    all_true_scaled = []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(DEVICE)
            outputs = model(X_batch)
            all_preds_scaled.extend(outputs.cpu().numpy())
            all_true_scaled.extend(y_batch.cpu().numpy().reshape(-1, 1))

    # --- 4. é€†ç¼©æ”¾ä»¥è·å¾—çœŸå®å€¼ ---
    predictions = scaler_y.inverse_transform(np.array(all_preds_scaled)).flatten()
    true_values = scaler_y.inverse_transform(np.array(all_true_scaled)).flatten()
    
    # --- 5. è®¡ç®—å›å½’æŒ‡æ ‡ ---
    mse = mean_squared_error(true_values, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_values, predictions)
    r2 = r2_score(true_values, predictions)

    metrics = {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2_score': r2,
        'test_set_size': len(true_values)
    }
    LOGGER.info(f"è¯„ä¼°æŒ‡æ ‡:\n{json.dumps(metrics, indent=4)}")

    # --- 6. ä¿å­˜æŠ¥å‘Š ---
    report_path = os.path.join(PATHS['results'], f"final_evaluation_report_{model_name}.json")
    with open(report_path, 'w') as f:
        json.dump(metrics, f, indent=4)
    LOGGER.success(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³ {report_path}")

    # --- 7. å¯è§†åŒ–ï¼šé¢„æµ‹å€¼ vs çœŸå®å€¼ ---
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei'] # æŒ‡å®šé»˜è®¤å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
    except Exception as e:
        LOGGER.warning(f"è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥ï¼Œå›¾è¡¨å¯èƒ½æ˜¾ç¤ºä¹±ç : {e}")

    plt.style.use('seaborn-v0_8-darkgrid')
    fig, ax = plt.subplots(figsize=(10, 10))
    sns.scatterplot(x=true_values, y=predictions, ax=ax, alpha=0.6, edgecolor='w')
    
    lims = [
        min(min(true_values), min(predictions)),
        max(max(true_values), max(predictions))
    ]
    ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='ç†æƒ³æƒ…å†µ (y=x)')
    
    ax.set_xlabel("çœŸå®å€¼ (True Values)", fontsize=12)
    ax.set_ylabel("é¢„æµ‹å€¼ (Predicted Values)", fontsize=12)
    ax.set_title(f"æ¨¡å‹è¯„ä¼°: é¢„æµ‹å€¼ vs. çœŸå®å€¼ ({model_name}) | RÂ² = {r2:.3f}", fontsize=16)
    ax.legend()
    ax.grid(True)
    
    plot_path = os.path.join(PATHS['results'], f"prediction_vs_true_{model_name}.png")
    plt.savefig(plot_path)
    LOGGER.success(f"è¯„ä¼°å›¾è¡¨å·²ä¿å­˜è‡³ {plot_path}")
    plt.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¯„ä¼°äº‹ä»¶é©±åŠ¨å›å½’æ¨¡å‹ã€‚")
    parser.add_argument(
        '--model-name', 
        type=str, 
        default='btc-crossover-regression-v1', 
        help='è¦è¯„ä¼°çš„æ¨¡å‹åç§°ã€‚'
    )
    args = parser.parse_args()
    run_evaluation(model_name=args.model_name) 