# btc_predictor/train.py

import os
import torch
import torch.nn as nn
import numpy as np
import joblib
import argparse
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import torch.optim as optim

from .config import get_model_config, PATHS
from .model import create_model
from .utils import LOGGER, setup_logger
from .data import get_prepared_data_for_training
import json

# --- å…¨å±€å¸¸é‡ ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def run_training(model_name: str):
    """
    ä¸ºæŒ‡å®šçš„æ¨¡å‹åç§°è¿è¡Œå®Œæ•´çš„è®­ç»ƒã€éªŒè¯å’Œä¿å­˜æµç¨‹ã€‚
    """
    setup_logger()
    LOGGER.info(f"ğŸš€ å¼€å§‹ä¸ºæ¨¡å‹ '{model_name}' è¿›è¡Œè®­ç»ƒ...")
    
    # --- 1. åŠ è½½å’Œå‡†å¤‡æ•°æ® ---
    try:
        model_config = get_model_config(model_name)
        task = model_config.get('task', 'regression')
        
        X, y = get_prepared_data_for_training(model_name)
        
        if X.empty or y.empty:
            LOGGER.error("æ²¡æœ‰å¯ä¾›è®­ç»ƒçš„æ•°æ®ã€‚")
            return

    except Exception as e:
        LOGGER.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return

    # --- 2. æ•°æ®æ‹†åˆ†å’Œç¼©æ”¾ ---
    tscv = TimeSeriesSplit(n_splits=5)
    
    # è·å–æœ€åä¸€ä¸ªæ‹†åˆ†ä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    for train_index, val_index in tscv.split(X):
        pass
    
    X_train, X_val = X.iloc[train_index, :], X.iloc[val_index,:]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    
    scaler_y = None
    if task == 'regression':
        scaler_y = StandardScaler()
        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
        y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))
    else: # åˆ†ç±»ä»»åŠ¡
        y_train_scaled = y_train.values.reshape(-1, 1)
        y_val_scaled = y_val.values.reshape(-1, 1)

    # --- 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ ---
    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train_scaled, dtype=torch.float32))
    val_dataset = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32), torch.tensor(y_val_scaled, dtype=torch.float32))
    
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

    # --- 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    try:
        params_path = os.path.join(PATHS['results'], f"best_params_{model_name}.json")
        if os.path.exists(params_path):
            with open(params_path, 'r') as f:
                hyperparams = json.load(f)
            LOGGER.info(f"ä» {params_path} åŠ è½½äº†ä¼˜åŒ–è¶…å‚æ•°ã€‚")
        else:
            hyperparams = {} # ä½¿ç”¨é»˜è®¤å‚æ•°
            LOGGER.warning("æœªæ‰¾åˆ°è¶…å‚æ•°æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹å‚æ•°ã€‚")

        model_params = {
            'input_dim': X_train.shape[1],
            'task': task,
            'model_type': model_config.get('model_type', 'transformer'),
            **hyperparams
        }
        model = create_model(**model_params).to(DEVICE)

        if task == 'regression':
            criterion = nn.MSELoss()
        else: # åˆ†ç±»ä»»åŠ¡
            criterion = nn.BCELoss()
        
        optimizer = optim.Adam(model.parameters(), lr=hyperparams.get('learning_rate', 0.001))

    except Exception as e:
        LOGGER.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # --- 5. è®­ç»ƒå¾ªç¯ ---
    best_val_loss = float('inf')
    epochs_no_improve = 0
    patience = 10

    for epoch in range(50): # å‡è®¾æœ€å¤šè®­ç»ƒ50ä¸ªå‘¨æœŸ
        model.train()
        total_train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # éªŒè¯
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
                outputs = model(batch_X)
                val_loss = criterion(outputs, batch_y)
                total_val_loss += val_loss.item()
        
        avg_val_loss = total_val_loss / len(val_loader)
        LOGGER.info(f"Epoch [{epoch+1}/50], è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, éªŒè¯æŸå¤±: {avg_val_loss:.6f}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), os.path.join(PATHS['models'], f"final_model_{model_name}.pth"))
            joblib.dump(scaler_X, os.path.join(PATHS['models'], f"{model_config['scaler_name']}_X.joblib"))
            if scaler_y:
                joblib.dump(scaler_y, os.path.join(PATHS['models'], f"{model_config['scaler_name']}_y.joblib"))
            LOGGER.info("éªŒè¯æŸå¤±æ”¹å–„ï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
        else:
            epochs_no_improve += 1
        
        if epochs_no_improve >= patience:
            LOGGER.info(f"éªŒè¯æŸå¤±è¿ç»­ {patience} ä¸ªå‘¨æœŸæœªæ”¹å–„ï¼Œè§¦å‘æ—©åœã€‚")
            break
            
    LOGGER.success(f"è®­ç»ƒå®Œæˆã€‚æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¿è¡ŒBTCé¢„æµ‹æ¨¡å‹çš„è®­ç»ƒæµç¨‹ã€‚")
    parser.add_argument("--model-name", type=str, required=True, help="è¦è®­ç»ƒçš„æ¨¡å‹çš„åç§°ï¼ˆåœ¨config.pyä¸­å®šä¹‰ï¼‰ã€‚")
    args = parser.parse_args()
    
    run_training(model_name=args.model_name) 